{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cf3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a36d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: datasets in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: librosa in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: jiwer in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (3.1.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (0.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (0.32.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from jiwer) (8.2.1)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from click>=8.1.8->jiwer) (0.4.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\anaconda3\\envs\\finetuning\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# You can run this in terminal OR create a setup script\n",
    "!pip install  datasets transformers librosa jiwer evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a5303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\finetuning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4a2990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 244\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, Audio\n",
    "\n",
    "def load_local_dataset(audio_dir, transcription_dir):\n",
    "    audio_files = sorted([f for f in os.listdir(audio_dir) if f.endswith('.mp3')])\n",
    "    transcription_files = sorted([f for f in os.listdir(transcription_dir) if f.endswith('.txt')])\n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        base_name = os.path.splitext(audio_file)[0]\n",
    "        txt_file = base_name + \".txt\"\n",
    "        \n",
    "        audio_path = os.path.join(audio_dir, audio_file)\n",
    "        transcription_path = os.path.join(transcription_dir, txt_file)\n",
    "\n",
    "        if os.path.exists(transcription_path):\n",
    "            with open(transcription_path, 'r', encoding='utf-8') as f:\n",
    "                transcription = f.read().strip()\n",
    "            dataset.append({\n",
    "                \"audio\": audio_path,\n",
    "                \"sentence\": transcription\n",
    "            })\n",
    "\n",
    "    return Dataset.from_list(dataset)\n",
    "\n",
    "# Load your train dataset\n",
    "train_audio_dir = r\"C:\\Users\\ASUS\\Desktop\\Whispher-Finetuning\\sindhi_data\\sindhi_data\\audio\"\n",
    "train_text_dir = r\"C:\\Users\\ASUS\\Desktop\\Whispher-Finetuning\\sindhi_data\\sindhi_data\\transcriptions\"\n",
    "train_dataset = load_local_dataset(train_audio_dir, train_text_dir)\n",
    "\n",
    "# Preprocess: convert to Audio column with sampling_rate\n",
    "train_dataset = train_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(\"Training samples:\", len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd342b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51865, 384, padding_idx=50257)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 384)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperSdpaAttention(\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=384, out_features=51865, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Load Whisper Tiny instead of Whisper Base\n",
    "model_name = \"openai/whisper-tiny\"  # This is the correct model for Whisper Tiny\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b659b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# model_name = \"openai/whisper-base\"\n",
    "\n",
    "# processor = WhisperProcessor.from_pretrained(model_name)\n",
    "# model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# # Move to GPU if available\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6bbe01c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 244/244 [00:06<00:00, 39.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # Audio -> log-Mel features\n",
    "    batch[\"input_features\"] = processor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_tensors=\"pt\",\n",
    "        language=\"sd\",\n",
    "        task=\"transcribe\"\n",
    "    ).input_features[0]\n",
    "\n",
    "    # Transcription -> tokenized labels\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"sentence\"],\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "\n",
    "    return batch\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "463db78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        if (labels[:, 0] == processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b10090b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_27284\\1665441860.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 04:01, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.456500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.349300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.556600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.072100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.724100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.858500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.738500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.457100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.399100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.376500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.204900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.168300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.174900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.070700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.038500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.041700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.035700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.057700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.062600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.026800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.031100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.019100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.019300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.014400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.019500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.012900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.010200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.015100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.012700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.008000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\finetuning\\lib\\site-packages\\transformers\\modeling_utils.py:3464: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=r\"C:\\Users\\ASUS\\Desktop\\whispher-tiny\\input\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.005,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=2,\n",
    "    save_total_limit=2,\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,  # using train as eval for now\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"ğŸš€ Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save the model & processor\n",
    "save_path = r\"C:\\Users\\ASUS\\Desktop\\whispher-tiny\\output\"\n",
    "trainer.save_model(save_path)\n",
    "processor.save_pretrained(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "400e59c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, 50259], [2, 50359], [3, 50363]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50358, 50359, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. If this is not desired, please set these values explicitly.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensLogitsProcessor'> to see related `.generate()` flags.\n",
      "A custom logits processor of type <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> has been passed to `.generate()`, but it was also created in `.generate()`, given its parameterization. The custom <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> will take precedence. Please check the docstring of <class 'transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor'> to see related `.generate()` flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ common_voice_sd_41397153.mp3 | WER: 0.4375\n",
      "   REF: Ú‡Ø§ Ø§Ù‡Ùˆ Ø§Ø³Ø§Ù† Ú©ÙŠ Ù»ÚŒØ§Ø¦ÙŠ Ù¿Ùˆ ØªÙ‡ Ø¢Ù…Ø±ÙŠÚªÙŠ ÙˆØ§Ù¾Ø§Ø±ÙŠ Ù‚Ø§Ù†ÙˆÙ† ÚªÙ… ÚªØ±ÙŠ Ø±Ù‡ÙŠÙˆ Ø¢Ù‡ÙŠØŒ Ù‡Ù† Ú†ÙŠÙˆ\n",
      "   HYP:  Ú†Ø§Ù† Ø§Ù‡Ùˆ Ø§Ø³Ø§Ù† Ú©ÙŠ Ø§ÚŒØ§Ø¦ÙŠ Ù¿Ùˆ ØªÙ‡ Ø¹Ù…Ø±ÙŠÙ‚ÙŠØŒ Ù¾Ø§Ù¾Ø§Ø±ÙŠ ÚªØ§Ù…Ù„ÙˆÙ† ÚªÙ… ÚªØ±ÙŠ Ø±Ù‡ÙŠÙˆ Ø¢Ù‡ÙŠØŒ Ù†Ù‡ Ù¿ÙŠÙˆ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397158.mp3 | WER: 0.7500\n",
      "   REF: Ø¨Ø®Ø§Ø±ÙŠ Ø¬ÙŠ Ù»ÙŠÙ† Ø±ÙˆØ§ÙŠØªÙ† Ù…Ø§Ù† Ù…Ø¹Ù„ÙˆÙ… Ù¿Ø¦ÙŠ Ù¿Ùˆ\n",
      "   HYP:  Ù»Ùˆ Ø®Ø§Ø±ÙŠ Ø¬ÙŠ Ø¨ÙŠÙ½Ù† Ø±ÙˆØ§Ø¦ÙŠÙ½Ù† Ù…Ø§Ù†ØŒ Ù…Ø¹Ù„ÙˆÙ… Ù¿Ø¦ÙŠ Ù¿Ùˆ. \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397194.mp3 | WER: 0.4000\n",
      "   REF: Ú‡Ù‡ÙŠÙ† Ù…Ø§Ù„Ø§ Ù¾ÙŠØ¯Ø§ Ù¿Ø¦ÙŠ Ù¿ÙŠ\n",
      "   HYP:  Ú†Ù‡Ù† Ù…Ø§Ù„Ø§ Ù¾ÙŠØ¯Ø§ Ù¿Ø¦ÙŠÙ½ÙŠØ¡Ù Ù¿ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397195.mp3 | WER: 0.5789\n",
      "   REF: ÙŠÙ‚ÙŠÙ†Ù†ØŒ Ø§Ù† Ø¬Ùˆ Ù…Ø·Ù„Ø¨ Ø§Ù‡Ùˆ Ù†Ù‡ ÙˆÙºÚ» Ú¯Ù‡Ø±Ø¬ÙŠ ØªÙ‡ Ù‡ØªÙŠ ÚªÙŠ Ø¨Ù‡ Ù…Ø³Ø¦Ù„Ø§ Û½ Ù…Ø­Ø±ÙˆÙ… Ø·Ø¨Ù‚Ù† Ø¬ÙŠ Ú©ÙˆÙ½ Ù†Ø§Ù‡ÙŠ.\n",
      "   HYP:  Ø§Ù‡ÚªÙŠÙ†Ù† Ø§Ú»Ù‡Ù† Ø§Ú„Ù‡Ù† Ø¬Ùˆ Ù…Ø·Ù„Ø¨ Ø§Ù‡Ùˆ Ù†Ù‡ ÙˆÙºÚ» Ú¯Ú¾Ø±Ø¬ÙŠ Ù‡Ù½ÙŠ Ù‡Ù½ÙŠ ÚªÙŠ Ø¨Ù‡ Ù…Ø³Ù„Ø§Ø­ Û½ Ù…Ø­Ø±ÙˆÙ† ØªØ¨Ù‚Ù† Ø¬ÙŠ Ø®Ùˆ ØªÙ†Ø§Ø­ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397199.mp3 | WER: 0.5000\n",
      "   REF: ØªÙ†Ù‡Ù† ÚªØ±ÙŠØŒ Ø§Ù†Ù‡Ù† Û¾ ØµØ§Ù„Ø­ Ø¢Ù‡Ù†\n",
      "   HYP:  ØªÙ‡Ù† ÚªØ±ÙŠ Ø§Ù†Ù‡Ù† Û¾ Ø³Ø§Ù„ÙŠ Ø¢Ù‡Ù† \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397200.mp3 | WER: 1.0000\n",
      "   REF: Ø§Ù‡ÙŠ Ø­Ø§Ù„ØªÙˆÙ† ØªÚÙ‡Ù† Ù‡ÙŠÙˆÙ†.\n",
      "   HYP:  Ú©Ø¦ÙŠ Ø­Ø§Ù„ØªÙˆ ØªÚÙŠ Ù‡Ø¦ÙŠÙˆÙ†. \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397487.mp3 | WER: 0.8889\n",
      "   REF: Ø­Ù…Ø§Ø¦Ù…Ù‡ Ù…Ù„Úª Ø¨Ù‡ ÙˆÙˆÙ…ÙŠÙ† Ù¾Ø±ÙˆÙ½ÙŠÚªØ´Ù† Ø¨Ù„ Ø¬ÙŠ Ø­Ø§Ù…ÙŠ Ø¢Ù‡ÙŠ\n",
      "   HYP:  Ù‡Ù… Ø§Ø¦ÙŠ ÙˆØ§Ù† Ù…Ù„Úª Ø¨Ù‡ ÙˆÙ…Ù†Ù‡Ù† Ù¾Ú™ÙŠØ´Ù† Ø¨Ù„ Ø¬ÙŠ Ù‡Ø§Ù† Û¾ Ø¢Ù‡ÙŠ. \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397489.mp3 | WER: 0.4706\n",
      "   REF: ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø³Ø§Ø¯Ú¯ÙŠ Û½ ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ ÙˆØ§Ù„Ø¯ÙŠÙ† Ø³Ø§Ù† ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø´ÙÙ‚Øª ÚØ³ÙŠ Ù…Ø§Ù† ÚØ§ÚÙˆ Ù…ØªØ§Ø«Ø± Ù¿ÙŠÙˆ Ø¢Ù‡ÙŠØ§Ù†\n",
      "   HYP:  ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø³Ø§ØªÚ¯ÙŠ Û½ ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ ÙˆØ§Ú³Ø§Ø¦Ù† Ø³Ø§Ù† ØªÙˆÙ‡Ø§Ù† Ø¬ÙŠ Ø´ÙÚªØªØ±Ø³ÙŠ ÙˆØ§Ø±ÙŠØ§ ÚÙˆ Ù…Ù¿Ø§Ø³ Ú†Ùˆ Ø¢Ù‡ÙŠØ§Ù† \n",
      "\n",
      "ğŸ“„ common_voice_sd_41397523.mp3 | WER: 0.7500\n",
      "   REF: Ø§Ø­Ø¯ Ú†ÙŠÙ…Ø§ Ú©ÙŠ ÙˆÙºÙˆ.\n",
      "   HYP:  Ø§Ù‡Ú™ Ù½ÙŠÙ…Ø§Ù† Ú©ÙŠ Ù…Ù¿Ùˆ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421602.mp3 | WER: 0.8333\n",
      "   REF: Ø®Ø´ÙˆÙ†Øª Ø³Ù†Ú¯Ù‡Ù‡ Ø¹Ù„Ø§Ù…Ù‡ Ø§Ù‚Ø¨Ø§Ù„ Ø¬ÙŠ ÚªÙ„Ø§Ù… Ø¬Ùˆ Ø§Ù†Ú¯Ø±ÙŠØ²ÙŠØ¡Ù Û¾ ØªØ±Ø¬Ù…Ùˆ ÚªÙŠÙˆ Ø¢Ù‡ÙŠ.\n",
      "   HYP:  Ù‡Ø´ÙˆÙ†Ù‡Ù½ Ø³Ù†Ú¯ Ø¹Ø±Ø§Ù… Ù…Ø§Ø¦Ù‚Ø¨Ø§Ø± Ø¬ÙŠ Ø®Ù„Ø§Ù… Ø¬ÙˆØŒ Ø¬Ùˆ Ø§Ù†Ú¯Ø±ÙŠØ²ÙŠ Û¾ ØªØ¬Ù…Ø¹ÙˆÙ† Ù¾ÙŠÙˆ Ø¢Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421678.mp3 | WER: 0.4000\n",
      "   REF: Ù‡ÙŠ Ø§Ù†Ù‡Ù† Ù…Ø§Ù† Ù‡Úª Ø¢Ù‡ÙŠ.\n",
      "   HYP:  Ù‡ÙŠ Ø§Ù†Ù‡Ù† ÙˆØ§Ù† Ù‡Úª Ø¢Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421686.mp3 | WER: 0.4706\n",
      "   REF: Ù…ÙˆÙ† Ø¹Ø§Ø¯Øª Ø¬ÙŠ Ø®Ù„Ø§Ù Ø³ÙŠÙ†Ù½Ø±Ù„ Ù„Ø§Úª Ú©ÙˆÙ„ÙŠÙˆ Û½ Ú©ÙŠØ³ Ø³Ø§Ù…Ù‡ÙˆÙ† ÙˆØ§Ø±ÙŠ Ø³ÙŠÙ½ ØªÙŠ ÙˆÙŠÙ‡Ú» Ø¬Ùˆ Ø§Ø´Ø§Ø±Ùˆ ÚªÙŠÙˆ\n",
      "   HYP:  Ù»ÙˆÙ† Ø¹Ø§Ø¯Øª Ù„ÙŠ Ú©Ù„Ø§Ù Ø³Ù†ÙŠÙ†Ù½Ø±Ù„ÙˆÚª Ú©ÙˆÙ„ÙŠÙˆ Û½ Ú©ÙŠØ³ Ø³Ø§Ù…Ù‡ÙˆÙ† ÙˆØ§Ø±ÙŠ Ø³ÙŠÙ½Øª ØªÙŠ ÙˆÙŠÙ‡Ù† Ø¬Ùˆ Ø´Ø§Ø±Ùˆ ÚªÙŠÙˆ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421762.mp3 | WER: 1.4000\n",
      "   REF: Ù¾Ø§Ú™ÙŠØ³Ø±ÙŠÙ† Ø³Ø§Ù† Ù¾Ø±Ø§Ù…Ù† Ø²Ù†Ø¯Ú¯ÙŠ Ú¯Ø°Ø§Ø±ÙŠÙˆ\n",
      "   HYP: Ù¾Ø§Ø±ÙŠ Ø³Ø±ÙŠÙ† Ø³Ø§Ù† Ù¾Ø± Ø§Ù…Ù† Ø²Ù† ØªÙ‚ÙŠ Ú¯Ø°Ø§Ø±ÙŠÙˆ. \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421798.mp3 | WER: 0.6000\n",
      "   REF: ÚªÚÙ‡Ù† Ú‡ÙˆÚªØ±ÙŠÙ† Ú©ÙŠ Ú¯ÙˆÙ„ÙŠÙˆÙ† Ù‡Ú»ÙŠ Ø²Ù†Ø¯Ù‡ Ø¯ÙÙ† ÚªÙŠÙˆ ÙˆÙŠÙ†Ø¯Ùˆ Ø¢Ù‡ÙŠ\n",
      "   HYP: ÚªÚÙ‡Ù† Ú†ÙˆÙ†ÚªØ±ÙŠÙ† Ú©ÙŠ Ù»ÙˆÙ„ÙŠÙˆÙ† Ù‡Ù†ÙŠ Ø¬Ù†Ø¯Ø§ Ø¯ÙÙ† ÚªÙŠÙˆÙ† ÙˆÙŠØ¯Ùˆ Ø¢Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421807.mp3 | WER: 0.7500\n",
      "   REF: Ø§Ù‡Ø§ ØµÙ„Ø§Ø­ÙŠØª Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ø¹Ø±ÙˆØ¬ ØªÙŠ Ù†Ø¸Ø± Ø§Ú†ÙŠ Ù¿ÙŠ\n",
      "   HYP:  ÙŠØ§ Ø³Ø±Ø§Ø¦ÙŠ ØªÙ¾Ù†Ø¬ÙŠ Ø§Ø±ÙˆÙ† Ø¬ØªÙŠ Ù†Ø¸Ø± Ø§Ú†ÙŠØ¦Ù† Ù¿ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41421870.mp3 | WER: 0.3750\n",
      "   REF: Ù‡Ù† Ø³ÙˆÚ† Ú©ÙŠ ØªØ¨Ø¯ÙŠÙ„ ÚªØ±Ú» Ø¬ÙŠ Ø¶Ø±ÙˆØ±Øª Ø¢Ù‡ÙŠ.\n",
      "   HYP:  Ù‡Ù† Ø³ÙˆÚ† Ú©ÙŠ ØªØ¨Ø¯ÙŠÙ„ ÚªÚ™Ø¬ÙŠ Ø¶Ø±ÙˆØ±Øª Ø¢Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425127.mp3 | WER: 0.5714\n",
      "   REF: Ø¢Ø³Ù½Ø±ÙŠÙ„ÙŠØ§ Ø®Ù„Ø§Ù Ù½ÙŠ Ù½ÙˆØ¦Ù†Ù½ÙŠ Ø§Ø³ÚªÙˆØ§ÚŠ Ø¬Ùˆ Ø§Ø¹Ù„Ø§Ù† ÚªØ±ÙŠ Ú‡ÚÙŠÙˆ Ø¹Ø§Ù…Ø± Ø¨Ù‡ÙŠØ± Ø¹Ù…Ø§Ø¯ Ø¬ÙŠ ÙˆØ§Ù¾Ø³ÙŠ\n",
      "   HYP:  Ù‡ÙˆÙ†Ø³Ù½Ø±ÙŠÙ„ÙŠØ§ Ú©Ù„Ø§Ø³ Ù½ÙŠÙ½Ù†Ù½ÙŠ Ø§Ø³ÚªÙˆØ§Ø± Ø¬Ùˆ Ø§Ø¹Ù„Ø§Ø¯ ÚªØ±ÙŠ Ú‡ÚÙŠÙˆ Ø¢Ù…Ù‡Ø± Ø¨ÙŠÙ‡Ø± Ø¹Ù…Ø§Ø¯ Ø¬ÙŠ ÙˆØ§Ù¾Ø³ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425190.mp3 | WER: 0.7778\n",
      "   REF: Ø§ÙŠØªØ±Ùˆ Ø®ÙˆØ´ Ù¿ÙŠÙˆ Ø¬Ùˆ Ù‡Ø§Ú»ÙŠ Ù…Ù„Ø§Ù‚Ø§Øª Ø¬Ø§ Ú†Ø§Ù†Ø³ ÙˆÚŒÙ†Ø¯Ø§.\n",
      "   HYP:  Ù‡ÙŠØªØ±Ùˆ Ø®ÙˆØ´ Ù¿ÙŠÙˆ Ø¬Ùˆ Ù‡Ø§Ù„ÙŠ Ù…Ù„Ø§ÚªØª Ø¬Ø§ Ú†Ø§Ù† Ø³Ùˆ ÙˆÚŒ Ù…Ø§ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425208.mp3 | WER: 0.5000\n",
      "   REF: Ù…ÙˆÙ† Ú©ÙŠ Ø§Ù‡Ø§ Ø¬Ø±Ø¦Øª Ù†Ù‡ Ù‡Ø¦ÙŠ ØªÙ‡ Ú©ÙŠØ³ Ø§Ù‡Ø§ Ø®Ø±Ø§Ø¨ Ø®Ø¨Ø± Ù»ÚŒØ§ÙŠØ§Ù†\n",
      "   HYP:  Ù…ÙˆÙ† Ú©ÙŠ Ø§Ù‡ Ø¬ÙˆØ±Øª Ù†Ù‡ÙˆØ¡Ù ØªÙ‡ Ú©ÙŠØ³ Ø§Ù‡Ø§ Ø®Ø±Ø§Ø¨ Ø®Ø¨Ø± Ù»ÚŒ Ø¢ÙŠØ§Ù† \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425246.mp3 | WER: 0.4286\n",
      "   REF: Ù‡Ù† Ú€ÙŠØ±ÙŠ Ø¨Ù‡ Ø§Ø³Ù„Ø§Ù… Ø¢Ø¨Ø§Ø¯ Ø¬Ùˆ Ù…Ù‚Ø§Ø¨Ù„Ùˆ Ø¢Ù‡ÙŠØŒ Ù¾Ø± Ù‡Ù† Ú€ÙŠØ±ÙŠ Ø¨Ù‡ Ù…Ù‚Ø§Ø¨Ù„Ùˆ Ø¢Ù‡ÙŠ\n",
      "   HYP:  Ù‡Ù† Ø¨Ú¾Ø±ÙŠ Ø¨Ù‡ Ø§Ø³Ù„Ø§Ù… ÙˆÙˆØ­Ø¯ Ø¬Ùˆ Ù…ÙˆÙ‚Ø¹Ø¨Ø±Ùˆ Ø¢Ù‡ÙŠØŒ Ù¾Ú™Ù‡Ù† Ù‡Ù† Ø¨Ú¾Ø±ÙŠ Ø¨Ù‡ Ù…ÙˆÙ‚Ø¹Ø¨Ø±Ùˆ Ø¢Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425265.mp3 | WER: 5.5455\n",
      "   REF: Ù…Ø§Ù† Ú‡Ø§ Ù¿Ùˆ Ú†ÙˆØ§Ù† ØªÙ‡ â€Ø§Ø³ÙŠÙ† ØªÙˆÚ©ÙŠ Ù‚ÙŠØ§Ù…Øª Û¾ ÙˆÙºÙŠ ÙˆÙŠÙ†Ø¯Ø§Ø³ÙŠÙ†â€œ.\n",
      "   HYP:  Ù…Ø§Ù† Ú†Ø§Ù† Ù¿Ùˆ Ú†ÙˆØ¡Ù ØªÙ‡ Ø§Ø³ÙŠÙ† ØªÙˆ Ú©ÙŠ ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ØªÙ‡ ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ØªÙ‡ ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚªÙŠØ¦Ù† ÚØ§Ø³ÙŠÙ†Ù‡Ù† \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425290.mp3 | WER: 1.0000\n",
      "   REF: Ø§ÙŠØ´ÙŠØ§\n",
      "   HYP:  Ù¾ÙŠØ´ÙŠØ§ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425296.mp3 | WER: 1.0000\n",
      "   REF: Ù…Ø«Ø§Ù„ ÚÙŠÙˆ.\n",
      "   HYP:  Ø¨Ø³Ø§Ù„ ÚÙŠÙˆ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425328.mp3 | WER: 1.0000\n",
      "   REF: Ø§Ù‡Ø§ Ø¢Ù‡ÙŠ Ø³Ø®Øª ØªÙ†Ù‚ÙŠØ¯.\n",
      "   HYP:  Ù‡ÙˆÙ‡ Ø¢Ù‡ÙŠ Ø³ÚªØª ØªÙ†Ù‚ÙŠ Ø¯Ù‡ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425350.mp3 | WER: 0.7727\n",
      "   REF: Ù…ÙˆÙ† Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ø¢Ø®Ø±ÙŠ Ù¾ÙˆØ³Ù½ Û¾ Ú†ÙŠÙˆ Ù‡Ùˆ ØªÙ‡ Ø§Ø³Ø§Ù† Ø¬ÙŠ Ù½ÙŠÙ… Ú©Ù½ÙŠ Ø±Ù‡ÙŠ Ø¢Ù‡ÙŠ Ù¾Ø± Ø§Ù† Û¾ Ú†ÙŠÙ…Ù¾ÙŠØ¦Ù† Ø¨Ú»Ø¬Ú» Ø¬Ùˆ Ù…Ø¹ÙŠØ§Ø± Ù†Ø§Ù‡ÙŠ\n",
      "   HYP:  Ù‡ÙˆÙ† Ù¾Ù†Ù‡Ù†Ø¬ÙŠ Ø¢Ù‡Ù†Ø¬ÙŠ Ø¢Ù‡Ù† Ø®Ù½ÙŠØ¡Ù Ø±Ù‡ÙŠ Ø¢Ù‡ÙŠØŒ Ù¾Ø± Ø§Ù† Û¾ Ú†Ù†Ù¾Ù† Ú€Ú„Ú» Ø¬ÙŠ Ø¢Ù‡Ù† Ø¬Ùˆ Ù…ÙŠØ§Ø± Ù†Ø§Ù‡ÙŠ \n",
      "\n",
      "ğŸ“„ common_voice_sd_41425395.mp3 | WER: 0.5000\n",
      "   REF: Ù»ÙŠÙˆ Ù…Ø§Ù„Ø§ Ù¾ÙŠØ¯Ø§ Ù¿ÙŠÙˆ\n",
      "   HYP:  Ù»ÙŠÙˆ Ù…Ø¹Ù„Ùˆ Ù¾ÙŠØ¯Ø§ Ú†Ùˆ \n",
      "\n",
      "âœ… Saved results to finetuned_whisper_sindhi_results.csv\n",
      "ğŸ”¥ Average WER on test set: 0.8731\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "import librosa\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "from evaluate import load\n",
    " \n",
    "# Load fine-tuned model and processor\n",
    "\n",
    "model_path = r\"C:\\Users\\ASUS\\Desktop\\Whispher-Finetuning\\final-model\"\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_path)\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# Fix the generation config - remove forced_decoder_ids\n",
    "\n",
    "model.generation_config.language = \"<|sd|>\"\n",
    "\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "model.generation_config.forced_decoder_ids = None  # This is the key fix\n",
    " \n",
    "# Load WER metric\n",
    "\n",
    "wer_metric = load(\"wer\")\n",
    " \n",
    "# Test data paths\n",
    "\n",
    "test_audio_dir = r\"C:\\Users\\ASUS\\Desktop\\Whispher-Finetuning\\test_54\\audio\"\n",
    "\n",
    "test_text_dir = r\"C:\\Users\\ASUS\\Desktop\\Whispher-Finetuning\\test_54\\transcription\"\n",
    " \n",
    "# Files to evaluate\n",
    "\n",
    "test_files = sorted([f for f in os.listdir(test_audio_dir) if f.endswith(\".mp3\")])\n",
    " \n",
    "# Inference loop\n",
    "\n",
    "results = []\n",
    "\n",
    "total_wer = 0\n",
    " \n",
    "for audio_file in test_files:\n",
    "\n",
    "    audio_path = os.path.join(test_audio_dir, audio_file)\n",
    "\n",
    "    text_path = os.path.join(test_text_dir, audio_file.replace(\".mp3\", \".txt\"))\n",
    " \n",
    "    # Load audio\n",
    "\n",
    "    audio, sr = librosa.load(audio_path, sr=16000)\n",
    "\n",
    "    input_features = processor(audio, sampling_rate=sr, return_tensors=\"pt\").input_features.to(model.device)\n",
    " \n",
    "    # Get reference text\n",
    "\n",
    "    with open(text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "\n",
    "        reference = f.read().strip().lower()\n",
    " \n",
    "    # Generate transcription with explicit generation parameters\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        predicted_ids = model.generate(\n",
    "\n",
    "            input_features,\n",
    "\n",
    "            language=\"<|sd|>\",  # Sindhi language token\n",
    "\n",
    "            task=\"transcribe\",\n",
    "\n",
    "            forced_decoder_ids=None,  # Explicitly set to None\n",
    "\n",
    "            max_length=448,\n",
    "\n",
    "            num_beams=1,\n",
    "\n",
    "            do_sample=False\n",
    "\n",
    "        )\n",
    "\n",
    "    prediction = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0].lower()\n",
    " \n",
    "    # WER calculation\n",
    "\n",
    "    wer = wer_metric.compute(predictions=[prediction], references=[reference])\n",
    "\n",
    "    total_wer += wer\n",
    " \n",
    "    print(f\"ğŸ“„ {audio_file} | WER: {wer:.4f}\")\n",
    "\n",
    "    print(\"   REF:\", reference)\n",
    "\n",
    "    print(\"   HYP:\", prediction, \"\\n\")\n",
    " \n",
    "    results.append({\n",
    "\n",
    "        \"File\": audio_file,\n",
    "\n",
    "        \"Reference\": reference,\n",
    "\n",
    "        \"Prediction\": prediction,\n",
    "\n",
    "        \"WER\": round(wer, 4)\n",
    "\n",
    "    })\n",
    " \n",
    "# Save to CSV\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.to_csv(\"finetuned_whisper_sindhi_results.csv\", index=False)\n",
    "\n",
    "print(f\"âœ… Saved results to finetuned_whisper_sindhi_results.csv\")\n",
    " \n",
    "# Print average WER\n",
    "\n",
    "average_wer = total_wer / len(results)\n",
    "\n",
    "print(f\"ğŸ”¥ Average WER on test set: {average_wer:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc588f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
